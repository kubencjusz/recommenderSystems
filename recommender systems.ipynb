{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems often involve applying some kind of an algorithm to the so called **user-item ratings matrix** of size $NxM$ where $N$ - number of users, $M$ - number of items. This matrix is sparse which is good since our goal is to recommend products to users and it would be nice to have something to recommend in the first place. The optimized implementation involves application of sparse matrices or dictionaries but for the purpose of this notebook I won't use sparce matrices. In real-life scenarios this is absolutely crucial as the full matrix won't fit into memory. For example YouTube is said to have over 1.3 billion users and over 7 billion videos (as of 03.2019) and the user-item matrix size is mindblowing. At the same time each of those users watched only a small fraction of all available videos so it's pointless to store so many missing values in a normal matrix.\n",
    "\n",
    "The value in a cell could be a rating (eg. on a scale from 1 to 5 - star-based system), whether a user rated an item positively or negatively (thumb-up / thumb-down approach). This type of information is called **explicit** - we know the real attitude of a particular user toward an item. The second type of information used in recommender systems is **implicit** rating - we only know that a user browsed an item, watched a movie, listened to a song etc. - in that case we can't tell if he/she liked it explicitly, we can only assume that. It is often concluded that the more times a user interacted with an item/song/movie the more confidence we have that he/she in fact likes it.\n",
    "\n",
    "The implementation presented below assumes we only deal with explicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| UserId \\ ItemId | Item 1 | Item 2 | Item 3 | Item 4 | ... | Item M |\n",
    "|:---------------:|--------|--------|--------|--------|-----|--------|\n",
    "| **User 1**          |    2   |    4   |    4   |    3   | ... |   4.5  |\n",
    "| **User 2**          |   3.5  |    5   |   4.5  |   2.5  | ... |    4   |\n",
    "| **User 3**          |    1   |    2   |    5   |   1.5  | ... |    5   |\n",
    "| **...**             |   ...  |   ...  |   ...  |   ...  | ... |   ...  |\n",
    "| **User N**          |    4   |   3.5  |    4   |   4.5  | ... |   2.5  |\n",
    "\n",
    "<div style=\"text-align: center\">Example of user-item ratings matrix</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import product\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the main goal of this notebook is to explain the intuition behind the most common algorithms related to recommender systems and by no means the efficiency of implementation, the dataset size is small and the algorithm itself could be optimized. It is based on well-known movielens dataset available [here](https://grouplens.org/datasets/movielens/). The dataset was filtered to include only the most popular movies (top 1000) and the most active users (5000 users). Thus the dimensionality of user-item matrix is $5000x1000$ but the algorithm would work equally on larger dataset as well (at the cost of longer training time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.read_csv('/home/kuba/Desktop/colab_filt_data/rating.csv', usecols=[0,1,2])\n",
    "\n",
    "N_USERS = 5000\n",
    "N_MOVIES = 1000\n",
    "\n",
    "# calculate 1000 most popular movies\n",
    "mov_chosen = all_ratings.groupby('movieId')['userId'].nunique().sort_values()\n",
    "mov_chosen = mov_chosen.iloc[-N_MOVIES:].index.values\n",
    "\n",
    "# calculate top 5000 users who rated the most movies\n",
    "user_chosen = all_ratings.groupby('userId')['movieId'].nunique().sort_values()\n",
    "user_chosen = user_chosen.iloc[-N_USERS:].index.values\n",
    "\n",
    "# filter a data frame to selected movies and users\n",
    "# we need to call copy() method, because selecting rows from dataframe only creates a view of original df\n",
    "# hence we have to make a new df by calling copy(). It lets us overwrite the indexes\n",
    "ratings = all_ratings[all_ratings.userId.isin(user_chosen) & all_ratings.movieId.isin(mov_chosen)].copy()\n",
    "del all_ratings\n",
    "\n",
    "# save the filtered data frame\n",
    "ratings.to_csv('/home/kuba/Desktop/colab_filt_data/ratings_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset ready we will split it into training and test sets (train-test split = 0.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1647602 distinct ratings in the training set.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>648382</th>\n",
       "      <td>37319</td>\n",
       "      <td>5266</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016091</th>\n",
       "      <td>59407</td>\n",
       "      <td>8376</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085030</th>\n",
       "      <td>122178</td>\n",
       "      <td>1387</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673590</th>\n",
       "      <td>38899</td>\n",
       "      <td>2717</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028128</th>\n",
       "      <td>60020</td>\n",
       "      <td>1199</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  rating\n",
       "648382    37319     5266     3.0\n",
       "1016091   59407     8376     2.5\n",
       "2085030  122178     1387     3.5\n",
       "673590    38899     2717     3.5\n",
       "1028128   60020     1199     4.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"/home/kuba/Desktop/colab_filt_data/new_ramka.csv\")\n",
    "random.seed(2019)\n",
    "\n",
    "# train-test-split: 70% split\n",
    "orig_ind = ratings.index.tolist()\n",
    "tr_ind = random.sample(orig_ind, int(0.7*ratings.shape[0]), )\n",
    "\n",
    "X_train = ratings.loc[tr_ind, :]\n",
    "X_test = ratings.loc[list(set(orig_ind)-set(tr_ind)), :]\n",
    "\n",
    "# what the training data look like?\n",
    "print(\"We have \" + str(X_train.shape[0]) + \" distinct ratings in the training set.\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert existing dataframe to a $NxM$ ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most active user watched 725 out of 1000 movies\n",
      "The most popular movie was watched by 3322 out of 5000 users\n"
     ]
    }
   ],
   "source": [
    "R_mat = X_train.pivot(index = 'userId', columns = 'movieId', values = 'rating')\n",
    "\n",
    "# we use a fact that pivot sorts indexes to create a mapping for movies and users\n",
    "user_map = dict(zip(range(N_USERS), R_mat.index.values))\n",
    "movies_map = dict(zip(range(N_MOVIES), R_mat.columns.values))\n",
    "user_map_rev = {v: k for k, v in user_map.items()}\n",
    "movies_map_rev = {v: k for k, v in movies_map.items()}\n",
    "\n",
    "# create numpy array - we could use scipy sparse matrices in case of larger matrix size\n",
    "R_mat = R_mat.values.astype(np.float64)\n",
    "R_mat_orig = R_mat.copy()\n",
    "print(\"The most active user watched \" + str(max(np.count_nonzero(~np.isnan(R_mat), 1))) +\n",
    "      \" out of \" + str(N_MOVIES) + \" movies.\")\n",
    "print(\"The most popular movie was watched by \" + str(max(np.count_nonzero(~np.isnan(R_mat), 0))) +\n",
    "      \" out of \" + str(N_USERS) + \" users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level the main goal of collaborative filtering is to predict what rating would user $i$ give to item $j$. Let's call the prediction $\\hat{r}(i,j)$ and the actual rating $r(i,j)$. Then we need to calculate mean squared error with a formula:\n",
    "$$MSE = \\frac{1}{|\\Omega|}\\sum_{i,j \\in \\Omega} (r_{ij} - \\hat{r}_{ij})^2$$\n",
    ", where $\\Omega$ is a set of all non-missing user-item rating pairs.\n",
    "\n",
    "Diving deeper, we need to calculate a set of similar users for each one of the users (by calculating correlation of ratings for each pair of users). Intuitively we should pay more attention to ratings of users who are more similar to a given user. When calculating a prediction for user $i$ and item $j$ we take into consideration the ratings given to item $j$ by all users who rated it (denoted by $\\Omega_j$) and weighting the result thanks to correlation factor between users (denoted by $w_{ii'}$) which we want to be high for similar users and low for dissimilar users:\n",
    "$$\\hat{r}(i,j) = \\frac{\\sum_{i' \\in \\Omega_j} w_{ii'}r_{i'j}}{\\sum_{i' \\in \\Omega_j} w_{ii'}}$$\n",
    "\n",
    "\n",
    "The other thing we need to care about is a naturally existing bias in ratings - there are users whose average rating is high (it's easy to make them happy) and very picky users (who rarely give high ratings and are generally hard to please). We need to accomodate for this fact by analyzing relative ratings. It is achieved by subtracting the mean rating of each user $i$ (denoted by $\\overline{r}_i$) from his/her ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most demanding user's average rating:[1.22659176]\n",
      "The least demanding user's average rating:[4.89194915]\n"
     ]
    }
   ],
   "source": [
    "# calculate average ratings for each user\n",
    "mean_ratings = np.nanmean(R_mat, axis=1).reshape(-1,1)\n",
    "print(\"The most demanding user's average rating:\" + str(min(mean_ratings)))\n",
    "print(\"The least demanding user's average rating:\" + str(max(mean_ratings)))\n",
    "\n",
    "# Subtract average ratings from known ratings.\n",
    "# We are interested in deviations of ratings from what is an average rating of a user.\n",
    "R_mat -= mean_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subtracted the average rating of each user from his/her known ratings. When making a prediction we need to account for this fact as well and the final prediction will be calculated as a sum of a given user's average rating ($\\overline{r}_i$ part) and the sum of deviations ($r_{i'j}-\\overline{r}_{i'}$ part) in ratings of item $j$ given by each user $i'$ who rated item $j$ ($\\Omega_j$ set) weighted by similarity of all pairs of users $(i, i')$ - $w_{i,i'} $ coefficients.\n",
    "$$\\hat{r}(i,j) = \\overline{r}_i + \\frac{\\sum_{i' \\in \\Omega_j} w_{i,i'}(r_{i'j}-\\overline{r}_{i'})}{\\sum_{i' \\in \\Omega_j} |w_{i,i'}|} $$\n",
    "\n",
    "The intuition behind this equation is that we need to add to our bias (how in general we rate items) the averaged differences between a rating of item $j$ that a user $i'$ gave and his/her bias. If a user $i'$ likes an item $j$ very much in comparison to his normal ratings and in addition he/she is very similar to us (large $w_{i,i'}$ coefficient) he/she will influence our score greatly.\n",
    "\n",
    "The final question arises - how do we compute correlation between users? We need to calculate ${N\\choose 2}$ pairs of correlations since $corr(i,i')=corr(i',i)$. It is a simple Pearson correlation but restricted to data we have:\n",
    "$$w_{i,i'} = \\frac{\\sum_{j \\in \\Omega_{i,i'}} (r_{ij}-\\overline{r}_i)(r_{i'j}-\\overline{r}_{i'})}{\\sqrt{\\sum_{j \\in \\Omega_i} (r_{ij}-\\overline{r}_i)^2} \\sqrt{\\sum_{j \\in \\Omega_{i'}} (r_{i'j}-\\overline{r}_{i'})^2}}$$, where:\n",
    "* $\\Omega_{i,i'}$ - set of items that both user $i$ and $i'$ have rated\n",
    "* $\\Omega_i$ - set of items that user $i$ has rated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution took: 0:00:17.313020\n",
      "1.0000000000000002\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# calculating Pearson correlactions between each pair of users\n",
    "us_range = range(N_USERS)\n",
    "weights = np.empty((N_USERS, N_USERS))\n",
    "\n",
    "all_pairs = list(product(us_range, us_range))\n",
    "# let's filter out the pairs to cut in half computation time (because of corr(x,y)=corr(y,x))\n",
    "all_pairs = [(x[0], x[1]) for x in all_pairs if x[0]>=x[1]]\n",
    "\n",
    "# instead of argwhere we could have created dicts: user2movie, movie2user, usermovie2rating\n",
    "# in order to lower computation time\n",
    "# looping through an array is O(N^2), looping through dict is O(|omega|), omega is set of all ratings\n",
    "t1 = datetime.now()\n",
    "for p1, p2 in all_pairs:\n",
    "    # which movies have both users watched?\n",
    "    mov1 = np.argwhere(~np.isnan(R_mat[p1, :]))\n",
    "    u1 = R_mat[p1, mov1]\n",
    "    mov2 = np.argwhere(~np.isnan(R_mat[p2, :]))\n",
    "    u2 = R_mat[p2, mov2]\n",
    "    mov_both = np.intersect1d(mov1, mov2)\n",
    "    u_both = R_mat[np.array([p1,p2])[:,None], mov_both]\n",
    "\n",
    "    # numerator\n",
    "    num = np.sum(u_both[0,:]*u_both[1,:])\n",
    "    # denominator\n",
    "    den = np.sqrt((u1**2).sum()) * np.sqrt((u2**2).sum())\n",
    "    sim = num / den\n",
    "    weights[p1, p2] = sim\n",
    "\n",
    "print(f'Execution took: {datetime.now()-t1}')\n",
    "\n",
    "# populate upper triangular matrix with transposition of lower triangular matrix\n",
    "i_upper = np.triu_indices(N_USERS, -1)\n",
    "weights[i_upper] = weights.T[i_upper]\n",
    "\n",
    "# sanity checks:\n",
    "print(weights[15, 15]) # must be equal to 1, since corr(x,x)=1\n",
    "print(weights[10, 15] == weights[15, 10]) # must return True, since corr(x,y)=corr(y,x)\n",
    "\n",
    "# save the weights for prediction\n",
    "np.save('/home/kuba/Desktop/colab_filt_data/weights', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select K top neighbors for each user\n",
    "N_NEIGH = 50\n",
    "neigh = {}\n",
    "for user in weights.index:\n",
    "    neigh[user] = weights.loc[user].sort_values(ascending=False).loc[:N_NEIGH]\n",
    "\n",
    "\n",
    "# PREDICTION\n",
    "def predict(i=10, j=23):\n",
    "    who_watched = np.argwhere(~np.isnan(R_mat[:, j]))\n",
    "    numer = (weights[i, who_watched].reshape(-1,1) * (R_mat[who_watched, j])).sum()\n",
    "    denom = (np.abs(weights[i, who_watched])).sum()\n",
    "    s_ij = mean_ratings[i] + numer / denom\n",
    "    # round extreme results so the range of ratings is <0.5;5>\n",
    "    s_ij = min(5, s_ij)\n",
    "    s_ij = max(0.5, s_ij)\n",
    "    return s_ij\n",
    "\n",
    "predict(i=0, j=0)\n",
    "\n",
    "# defining MSE function\n",
    "def MSE(true, pred):\n",
    "    return np.sum((true-pred)**2) / len(pred)\n",
    "\n",
    "\n",
    "# calculating MSE on train set, which pairs to choose?\n",
    "train_pred_pairs = []\n",
    "for ind, row in X_train.iterrows():\n",
    "    tmp = (user_map_rev[row['userId']], movies_map_rev[row['movieId']])\n",
    "    train_pred_pairs.append(tmp)\n",
    "\n",
    "# calculate score for train set, for 10k ratings only\n",
    "train_scores = np.array([predict(x, y) for x, y in train_pred_pairs[:10000]]).reshape(-1,1)\n",
    "true_train = X_train['rating'].values.reshape(-1,1)\n",
    "train_MSE = MSE(np.array(true_train[:10000]), train_scores)\n",
    "print(\"Train MSE: \" + str(train_MSE))\n",
    "\n",
    "# calculating MSE on test set\n",
    "test_pred_pairs = []\n",
    "for ind, row in X_test.iterrows():\n",
    "    tmp = (user_map_rev[row['userId']], movies_map_rev[row['movieId']])\n",
    "    test_pred_pairs.append(tmp)\n",
    "\n",
    "test_scores = [predict(x, y) for x, y in test_pred_pairs]\n",
    "test_MSE = MSE(X_test['rating'].values.tolist(), test_scores)\n",
    "print(\"Test MSE: \" + str(test_MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another algorithm widely used in a domain of recommender systems is matrix factorization. It is still applied on the same user-item ratings matrix as presented in case of collaborative filtering ($N$ users and $M$ items). The goal is to find two matrices $W$ and $U$ whose dot product yields the initial user-item matrix\n",
    "$$R=WU^T$$\n",
    ", with the following dimensionalities:\n",
    "* $R$ is of shape $NxM$\n",
    "* $W$ is of shape $NxK$\n",
    "* $U$ is of shape $MxK$\n",
    "\n",
    "However it is only possible the get the exact same matrix if $K=N$. In practice $K$ ranges from 10 to 50 and we get the best possible approximation of matrix $R$.\n",
    "$$\\hat{R} \\approx WU^T$$\n",
    "\n",
    "$K$ is a number of hidden factors, features that explain differences between items. In case of movies these could represent for example whether a movie:\n",
    "* is comedy, drama or horror\n",
    "* is black and white\n",
    "* features a specific actor, etc.\n",
    "\n",
    "We can identify these features by analyzing the items which score particularly high/low for a given feature.\n",
    "\n",
    "We should at all cost refrain from multiplying full matrices $W$ and $U$ when making prediction because we only need one row and one column in order to predict:\n",
    "$$\\hat{r}_{ij} = w_i^Tu_j$$\n",
    "The intuition behind this equation is the following: the product of two vectors $w_i$ and $u_j$ tells us how much user's $i$ preferences correlate with item's $j$ attributes. Let's pretend we have 5 hidden features: $K=5$ which can mean for example:\n",
    "* feature 1: Is di Caprio in the cast?\n",
    "* feature 2: Did the movie get any Oscars?\n",
    "* feature 3: Are there any superheroes involved?\n",
    "* feature 4: Is it a comedy?\n",
    "* feature 5: Is there a thread about romance?\n",
    "\n",
    "Our user $i$ has preferences which are described by vector $w_i = [1, 0.7, -0.9, 0.2, 1]$. This user watched movies *Titanic* and *Iron Man* whose attributes are described by vectors $u_{titanic}=[0.9, 1.7, -1.1, 0, 1.3]$ and $u_{ironman}=[-0.9, -1, 1, 0, -0.8]$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User i will probably like Titanic because the score is 4.38\n",
      "User i will probably dislike Iron Man because the score is -3.3000000000000003\n"
     ]
    }
   ],
   "source": [
    "usr = np.array([1, 0.7, -0.9, 0.2, 1])\n",
    "mov_titanic = np.array([0.9, 1.7, -1.1, 0, 1.3])\n",
    "mov_ironman = np.array([-0.9, -1, 1, 0, -0.8])\n",
    "\n",
    "print(\"User i will probably like Titanic because the score is \" + str(usr.dot(mov_titanic)))\n",
    "print(\"User i will probably dislike Iron Man because the score is \" + str(usr.dot(mov_ironman)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is how to calculate matrices $W$ and $U$? We want to minimize differences between elements of matrix $R$ and $\\hat{R}$ which gives us the following cost function:\n",
    "$$J = \\sum_{i,j \\in \\Omega} (r_{ij} - \\hat{r}_{ij})^2 =  \\sum_{i,j \\in \\Omega} (r_{ij} - w_i^Tu_j)^2$$\n",
    ", where $\\Omega_{ij}$ is a set of all pairs where user $i$ rated a movie $j$.\n",
    "\n",
    "Similarly to collaborative filtering we need to include a user as well as a movie bias - there are users who generally rate movies high/low and there are movies which are generally liked/disliked. The user bias will be denoted as $b_i$ and movie bias as $c_j$:\n",
    "$$\\hat{r}_{ij} = w_i^Tu_j+b_i+c_j$$\n",
    "The final cost function to optimize is thus equal to:\n",
    "$$J = \\sum_{i,j \\in \\Omega} (r_{ij} - w_i^Tu_j - b_i - c_j)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a minimum of such a function we take a derivative of $J$ with respect to our parameters which in this case are elements of matrices $w_i$ and $u_j$. The derivation of $w_i$ is calculated as follows:\n",
    "$$\\frac{\\partial J}{\\partial w_i} = 2\\sum_{j \\in \\Omega_i} (r_{ij} - w_i^Tu_j - b_i - c_j)(-u_j)=0$$\n",
    ", where $j \\in \\Omega_i$ signifies a set of all movies $j$ that user $i$ has watched since elements of $w_i$ depend only on those values.\n",
    "$$\\sum_{j \\in \\Omega_i}(w_i^Tu_j)u_j=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "The product of vectors is commutative (i.e. $a\\cdot b=b\\cdot a$) so we can further transform the equation:\n",
    "$$\\sum_{j \\in \\Omega_i}(u_j^Tw_i)u_j=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "The expression on the left-hand side in parenthesis is a scalar and since $scalar \\cdot vector = vector \\cdot scalar$ we have:\n",
    "$$\\sum_{j \\in \\Omega_i}u_j(u_j^Tw_i)=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "On the left-hand side the $w_i$ factor doesn't depend on the summation index $j$ so we simplify:\n",
    "$$(\\sum_{j \\in \\Omega_i}u_ju_j^T)w_i=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all it takes to calculate $w_i$ is to multiply both sides by the inverse of $\\sum_{j \\in \\Omega_i}u_ju_j^T$ and the result is:\n",
    "$$w_i=(\\sum_{j \\in \\Omega_i}u_ju_j^T)^{-1}\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "\n",
    "Similar derivation may be performed for updates of vectors $u_j$, $b_i$ and $c_j$:\n",
    "$$u_i=(\\sum_{i \\in \\Omega_j}w_iw_i^T)^{-1}\\sum_{i \\in \\Omega_j}(r_{ij}-b_i -c_j )w_i$$\n",
    "$$b_i=\\frac{1}{|\\Omega_i|} \\sum_{j \\in \\Omega_i} (r_{ij}-w_i^Tu_j-c_j)$$\n",
    "$$c_j=\\frac{1}{|\\Omega_j|} \\sum_{i \\in \\Omega_j} (r_{ij}-w_i^Tu_j-b_i)$$\n",
    "\n",
    "As $w_i$ depends on $u_j$ and vice versa the optimization of such a function may be achieved with **Alternating Least Squares (ALS)**. The name of a method comes from the fact that we will update parameters of $U$ and $W$ in turns (alternate between updates). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting K - latent dimensionality\n",
    "K = 15\n",
    "lamb = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictionaries\n",
    "user2mov = {}\n",
    "mov2user = {}\n",
    "usermov2rating = {}\n",
    "\n",
    "for ind, us in user_map.items():\n",
    "    user2mov[ind] = [movies_map_rev[x] for x in ratings.loc[ratings.userId==us, 'movieId'].tolist()]\n",
    "\n",
    "for ind, mo in movies_map.items():\n",
    "    mov2user[ind] = [user_map_rev[x] for x in ratings.loc[ratings.movieId==mo, 'userId'].tolist()]\n",
    "\n",
    "for ind, row in ratings.iterrows():\n",
    "    usermov2rating[(user_map_rev[int(row.userId)], movies_map_rev[int(row.movieId)])] = row.rating\n",
    "    \n",
    "print(f'user2mov dict: {np.round(sys.getsizeof(user2mov)/1024**2,2)} MB occupied')\n",
    "print(f'mov2user dict: {np.round(sys.getsizeof(mov2user)/1024**2,2)} MB occupied')\n",
    "print(f'usermov2rating dict: {np.round(sys.getsizeof(usermov2rating)/1024**2,2)} MB occupied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing values\n",
    "W = np.random.randn(N_USERS, K)\n",
    "b = np.random.randn(N_USERS, 1)\n",
    "U = np.random.randn(N_MOVIES, K)\n",
    "c = np.random.randn(N_MOVIES, 1)\n",
    "miu = X_train.rating.mean()\n",
    "\n",
    "# alternating least squares algorithm\n",
    "k = True\n",
    "train_loss = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while k:\n",
    "    # update W matrix\n",
    "    for us, us_m in user2mov.items():\n",
    "        mat = U[us_m].T.dot(U[us_m]) + lamb * np.eye(K)\n",
    "        us_mov_pairs = list(zip([us]*len(us_m), us_m))\n",
    "        vec = np.array([(usermov2rating[(x, y)] - b[x] - c[y] - miu)*(U[y].T) for x, y in us_mov_pairs]).sum(0)\n",
    "        W[us, :] = np.linalg.solve(mat, vec) # nie wiem czemu tu przypisuje, a nie robie -=\n",
    "        # to nie jest gradient?\n",
    "        \n",
    "        # update bias b\n",
    "        denom = 1.0 / (len(us_m) + lamb)\n",
    "        numer = np.sum([usermov2rating[(us, mov)] - W[us].dot(U[mov].T) - c[mov]-miu for mov in us_m])\n",
    "        b[us] = numer*denom\n",
    "    \n",
    "    # update U matrix\n",
    "    for mo, mo_u in mov2user.items():\n",
    "        mat = W[mo_u].T.dot(W[mo_u]) + lamb * np.eye(K)\n",
    "        mov_us_pairs = list(zip([mo]*len(mo_u), mo_u))\n",
    "        vec = np.array([(usermov2rating[(y, x)] - b[y] - c[x] - miu)*(W[y].T) for x, y in mov_us_pairs]).sum(0)\n",
    "        U[mo, :] = np.linalg.solve(mat, vec)\n",
    "        \n",
    "        # update bias c\n",
    "        denom = 1.0 / (len(mo_u) + lamb)\n",
    "        numer = np.sum([usermov2rating[(us, mo)] - W[us].dot(U[mo].T) - b[us]-miu for us in mo_u])\n",
    "        c[mo] = numer*denom\n",
    "    \n",
    "    # calculate MSE\n",
    "    \n",
    "    if cond:\n",
    "        k = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
