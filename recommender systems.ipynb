{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems often involve applying some kind of an algorithm to the so called **user-item ratings matrix** of size $NxM$ where $N$ - number of users, $M$ - number of items. This matrix is sparse which is good since our goal is to recommend products to users and it would be nice to have something to recommend in the first place. The optimized implementation involves application of sparse matrices or dictionaries but for the purpose of this notebook I won't use sparse matrices. In real-life scenarios this is absolutely crucial as the full matrix won't fit into memory. For example YouTube is said to have over 1.3 billion users and over 7 billion videos (as of 03.2019) and the user-item matrix size is mindblowing. At the same time each of those users watched only a small fraction of all available videos so it's pointless to store so many missing values in a normal matrix.\n",
    "\n",
    "The value in a cell could be a rating (eg. on a scale from 1 to 5 - star-based system), whether a user rated an item positively or negatively (thumb-up / thumb-down approach). This type of information is called **explicit** - we know the real attitude of a particular user toward an item. The second type of information used in recommender systems is **implicit** rating - we only know that a user browsed an item, watched a movie, listened to a song etc. - in that case we can't tell if he/she liked it explicitly, we can only assume that. It is often concluded that the more times a user interacted with an item/song/movie the more confidence we have that he/she in fact likes it.\n",
    "\n",
    "The implementation presented below assumes we only deal with explicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| UserId \\ ItemId | Item 1 | Item 2 | Item 3 | Item 4 | ... | Item M |\n",
    "|:---------------:|--------|--------|--------|--------|-----|--------|\n",
    "| **User 1**          |    2   |    4   |    4   |    3   | ... |   4.5  |\n",
    "| **User 2**          |   3.5  |    5   |   4.5  |   2.5  | ... |    4   |\n",
    "| **User 3**          |    1   |    2   |    5   |   1.5  | ... |    5   |\n",
    "| **...**             |   ...  |   ...  |   ...  |   ...  | ... |   ...  |\n",
    "| **User N**          |    4   |   3.5  |    4   |   4.5  | ... |   2.5  |\n",
    "\n",
    "<div style=\"text-align: center\">Example of user-item ratings matrix</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_USERS = 5000\n",
    "N_MOVIES = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the main goal of this notebook is to explain the intuition behind the most common algorithms related to recommender systems and by no means the efficiency of implementation, the dataset size is small and the algorithm itself could be further optimized (I vectorized the code where possible though). It is based on the well-known movielens dataset available [here](https://grouplens.org/datasets/movielens/) (Analysis was perofrmed on the 20M records version). The dataset was filtered to include only the most popular movies (top 1000) and the most active users (5000 users). Thus the dimensionality of user-item matrix is $5000x1000$ but the algorithm would work equally on larger dataset as well (at the cost of longer training time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.read_csv('/home/kuba/Desktop/colab_filt_data/rating.csv', usecols=[0,1,2])\n",
    "\n",
    "# calculate 1000 most popular movies\n",
    "mov_chosen = all_ratings.groupby('movieId')['userId'].nunique().sort_values()\n",
    "mov_chosen = mov_chosen.iloc[-N_MOVIES:].index.values\n",
    "\n",
    "# calculate top 5000 users who rated the most movies\n",
    "user_chosen = all_ratings.groupby('userId')['movieId'].nunique().sort_values()\n",
    "user_chosen = user_chosen.iloc[-N_USERS:].index.values\n",
    "\n",
    "# filter a data frame to the cohsen movies and users\n",
    "# we need to call copy() method, because selecting rows from dataframe\n",
    "# only creates a view of original df hence we have to make a new df by calling copy()\n",
    "ratings = all_ratings[all_ratings.userId.isin(user_chosen) & all_ratings.movieId.isin(mov_chosen)].copy()\n",
    "del all_ratings\n",
    "\n",
    "# save the filtered data frame\n",
    "ratings.to_csv('/home/kuba/Desktop/colab_filt_data/ratings_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset ready, we will split it into training and test sets (train-test split = 0.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  1647602  distinct ratings in the training set.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>648382</th>\n",
       "      <td>37319</td>\n",
       "      <td>5266</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016091</th>\n",
       "      <td>59407</td>\n",
       "      <td>8376</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085030</th>\n",
       "      <td>122178</td>\n",
       "      <td>1387</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673590</th>\n",
       "      <td>38899</td>\n",
       "      <td>2717</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028128</th>\n",
       "      <td>60020</td>\n",
       "      <td>1199</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  rating\n",
       "648382    37319     5266     3.0\n",
       "1016091   59407     8376     2.5\n",
       "2085030  122178     1387     3.5\n",
       "673590    38899     2717     3.5\n",
       "1028128   60020     1199     4.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"./ratings_filtered.csv\") #(\"/home/kuba/Desktop/colab_filt_data/new_ramka.csv\")\n",
    "random.seed(2019)\n",
    "\n",
    "# train-test-split: 70% split\n",
    "orig_ind = ratings.index.tolist()\n",
    "tr_ind = random.sample(orig_ind, int(0.7*ratings.shape[0]), )\n",
    "\n",
    "X_train = ratings.loc[tr_ind, :]\n",
    "X_test = ratings.loc[list(set(orig_ind)-set(tr_ind)), :]\n",
    "\n",
    "# what the training data look like?\n",
    "print(\"We have \", X_train.shape[0], \" distinct ratings in the training set.\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the existing dataframe to a $NxM$ ratings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most active user watched 725 out of 1000 most popular movies.\n",
      "The most popular movie was watched by 3322 out of 5000 most active users.\n"
     ]
    }
   ],
   "source": [
    "R_mat = X_train.pivot(index = 'userId', columns = 'movieId', values = 'rating')\n",
    "\n",
    "# we use a fact that pivot sorts indexes to create a mapping for movies and users\n",
    "user_map = dict(zip(range(N_USERS), R_mat.index.values))\n",
    "movies_map = dict(zip(range(N_MOVIES), R_mat.columns.values))\n",
    "user_map_rev = {v: k for k, v in user_map.items()}\n",
    "movies_map_rev = {v: k for k, v in movies_map.items()}\n",
    "\n",
    "# create numpy array - we could use scipy sparse matrices in case of larger matrix size\n",
    "R_mat = R_mat.values.astype(np.float64)\n",
    "#R_mat_orig = R_mat.copy()\n",
    "\n",
    "print(\"The most active user watched\", max(np.count_nonzero(~np.isnan(R_mat), 1)),\n",
    "      \"out of\", N_MOVIES, \"most popular movies.\")\n",
    "print(\"The most popular movie was watched by\", max(np.count_nonzero(~np.isnan(R_mat), 0)),\n",
    "      \"out of\", N_USERS, \"most active users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a high level the main goal of collaborative filtering is to predict what rating would user $i$ give to item $j$. Let's call the prediction $\\hat{r}(i,j)$ and the actual rating $r(i,j)$. Then we need to calculate mean squared error with a formula:\n",
    "$$MSE = \\frac{1}{|\\Omega|}\\sum_{i,j \\in \\Omega} (r_{ij} - \\hat{r}_{ij})^2$$\n",
    ", where $\\Omega$ is a set of all non-missing user-item rating pairs.\n",
    "\n",
    "Diving deeper, we need to calculate a set of similar users for each one of the users (by calculating correlation of ratings for each pair of users). Intuitively we should pay more attention to ratings of users who are more similar to a given user. When calculating a prediction for user $i$ and item $j$ we take into consideration the ratings given to item $j$ by all users who rated it (denoted by $\\Omega_j$) and weighting the result with a correlation factor between users (denoted by $w_{ii'}$) which we want to be high for similar users and low for dissimilar users:\n",
    "$$\\hat{r}(i,j) = \\frac{\\sum_{i\\prime \\in \\Omega_j} w_{ii\\prime}r_{i\\prime j}}{\\sum_{i\\prime \\in \\Omega_j} w_{ii\\prime}}$$\n",
    "\n",
    "\n",
    "The other thing we need to care about is a naturally existing bias in ratings - there are users whose average rating is high (it's easy to make them happy) and very picky users (who rarely give high ratings and are generally hard to please). We need to accomodate for this fact by analyzing relative ratings. It is achieved by subtracting the mean rating of each user $i$ (denoted by $\\overline{r}_i$) from his/her ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most demanding user's average rating: 1.23\n",
      "The least demanding user's average rating: 4.89\n"
     ]
    }
   ],
   "source": [
    "# calculate average ratings for each user\n",
    "mean_ratings = np.nanmean(R_mat, axis=1).reshape(-1,1)\n",
    "print(\"The most demanding user's average rating:\", np.round(min(mean_ratings)[0], 2))\n",
    "print(\"The least demanding user's average rating:\", np.round(max(mean_ratings)[0], 2))\n",
    "\n",
    "# Subtract average ratings from known ratings.\n",
    "# We are interested in deviations of ratings from what is an average rating of a user.\n",
    "R_mat -= mean_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subtracted the average rating of each user from his/her known ratings. When making a prediction we need to account for this fact as well and the final prediction will be calculated as a sum of a given user's average rating ($\\overline{r}_i$ part) and the sum of deviations ($r_{i\\prime j}-\\overline{r}_{i\\prime}$ part) in ratings of item $j$ given by each user $i\\prime$ who rated item $j$ ($\\Omega_j$ set) weighted by similarity of all pairs of users $(i, i\\prime)$ - $w_{i,i\\prime} $ coefficients.\n",
    "$$\\hat{r}(i,j) = \\overline{r}_i + \\frac{\\sum_{i\\prime \\in \\Omega_j} w_{i,i\\prime}(r_{i\\prime j}-\\overline{r}_{i\\prime})}{\\sum_{i\\prime \\in \\Omega_j} |w_{i,i\\prime}|} $$\n",
    "\n",
    "The intuition behind this equation is that we need to add to our bias (how in general we rate items) the averaged differences between a rating of item $j$ that a user $i\\prime$ gave and his/her bias. If a user $i\\prime$ likes an item $j$ very much in comparison to his normal ratings and in addition he/she is very similar to us (large $w_{i,i\\prime}$ coefficient) he/she will influence our score greatly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final question arises - how do we compute correlation between users? We need to calculate ${N\\choose 2}$ pairs of correlations since $corr(i,i\\prime)=corr(i\\prime,i)$. It is a simple Pearson correlation but restricted to data we have:\n",
    "$$w_{i,i\\prime} = \\frac{\\sum_{j \\in \\Omega_{i,i\\prime}} (r_{ij}-\\overline{r}_i)(r_{i\\prime j}-\\overline{r}_{i\\prime})}{\\sqrt{\\sum_{j \\in \\Omega_i} (r_{ij}-\\overline{r}_i)^2} \\sqrt{\\sum_{j \\in \\Omega_{i\\prime}} (r_{i\\prime j}-\\overline{r}_{i\\prime})^2}}$$, where:\n",
    "* $\\Omega_{i,i\\prime}$ - set of items that both user $i$ and $i\\prime$ have rated\n",
    "* $\\Omega_i$ - set of items that user $i$ has rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing shit\n",
      "Starting index 0 - previous took 0:00:00.003488\n",
      "Starting index 100 - previous took 0:00:00.668562\n",
      "Starting index 200 - previous took 0:00:01.997038\n",
      "Starting index 300 - previous took 0:00:03.331714\n",
      "Starting index 400 - previous took 0:00:04.738789\n",
      "Starting index 500 - previous took 0:00:05.992617\n",
      "Starting index 600 - previous took 0:00:07.382275\n",
      "Starting index 700 - previous took 0:00:08.565068\n",
      "Starting index 800 - previous took 0:00:09.990160\n",
      "Starting index 900 - previous took 0:00:11.236932\n",
      "Starting index 1000 - previous took 0:00:12.534618\n",
      "Starting index 1100 - previous took 0:00:13.726644\n",
      "Starting index 1200 - previous took 0:00:14.964427\n",
      "Starting index 1300 - previous took 0:00:17.759136\n",
      "Starting index 1400 - previous took 0:00:22.295429\n",
      "Starting index 1500 - previous took 0:00:35.640279\n",
      "Starting index 1600 - previous took 0:00:25.288003\n",
      "Starting index 1700 - previous took 0:00:21.772885\n",
      "Starting index 1800 - previous took 0:00:22.605239\n",
      "Starting index 1900 - previous took 0:00:29.108181\n",
      "Starting index 2000 - previous took 0:00:30.197024\n",
      "Starting index 2100 - previous took 0:00:26.968980\n",
      "Starting index 2200 - previous took 0:00:28.125377\n",
      "Starting index 2300 - previous took 0:00:33.279907\n",
      "Starting index 2400 - previous took 0:00:35.017353\n",
      "Starting index 2500 - previous took 0:00:33.413086\n",
      "Starting index 2600 - previous took 0:00:33.238887\n",
      "Starting index 2700 - previous took 0:00:34.495320\n",
      "Starting index 2800 - previous took 0:00:35.850191\n",
      "Starting index 2900 - previous took 0:00:37.177421\n",
      "Starting index 3000 - previous took 0:00:38.309369\n",
      "Starting index 3100 - previous took 0:00:39.607099\n",
      "Starting index 3200 - previous took 0:00:41.589733\n",
      "Starting index 3300 - previous took 0:00:42.355775\n",
      "Starting index 3400 - previous took 0:00:44.055791\n",
      "Starting index 3500 - previous took 0:00:46.467775\n",
      "Starting index 3600 - previous took 0:00:54.877113\n",
      "Starting index 3700 - previous took 0:00:47.633743\n",
      "Starting index 3800 - previous took 0:00:49.016978\n",
      "Starting index 3900 - previous took 0:00:50.277204\n",
      "Starting index 4000 - previous took 0:00:51.273275\n",
      "Starting index 4100 - previous took 0:00:52.672439\n",
      "Starting index 4200 - previous took 0:00:54.687833\n",
      "Starting index 4300 - previous took 0:00:55.877596\n",
      "Starting index 4400 - previous took 0:00:56.702772\n",
      "Starting index 4500 - previous took 0:00:57.946119\n",
      "Starting index 4600 - previous took 0:00:59.286830\n",
      "Starting index 4700 - previous took 0:01:01.522944\n",
      "Starting index 4800 - previous took 0:01:01.618478\n",
      "Starting index 4900 - previous took 0:01:03.339656\n",
      "1.0000000000000002\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# calculating Pearson correlactions between each pair of users\n",
    "us_range = range(N_USERS)\n",
    "weights = np.empty((N_USERS, N_USERS))\n",
    "\n",
    "all_pairs = list(product(us_range, us_range))\n",
    "# let's filter out the pairs to cut in half computation time (because of corr(x,y)=corr(y,x))\n",
    "all_pairs = [(x[0], x[1]) for x in all_pairs if x[0]>=x[1]]\n",
    "\n",
    "# instead of argwhere we could have created dicts: user2movie, movie2user, usermovie2rating\n",
    "# in order to lower computation time\n",
    "# looping through an array is O(N^2), looping through dict is O(|omega|), omega is set of all ratings\n",
    "t1 = datetime.now()\n",
    "for p1, p2 in all_pairs:\n",
    "    # which movies have both users watched?\n",
    "    mov1 = np.argwhere(~np.isnan(R_mat[p1, :]))\n",
    "    u1 = R_mat[p1, mov1]\n",
    "    mov2 = np.argwhere(~np.isnan(R_mat[p2, :]))\n",
    "    u2 = R_mat[p2, mov2]\n",
    "    mov_both = np.intersect1d(mov1, mov2)\n",
    "    u_both = R_mat[np.array([p1,p2])[:,None], mov_both]\n",
    "\n",
    "    # numerator\n",
    "    num = np.sum(u_both[0,:]*u_both[1,:])\n",
    "    # denominator\n",
    "    den = np.sqrt((u1**2).sum()) * np.sqrt((u2**2).sum())\n",
    "    sim = num / den\n",
    "    weights[p1, p2] = sim\n",
    "    \n",
    "    if p1 % 100==0 and p2==0:\n",
    "        print(\"Starting index\", p1, \"- previous took\", datetime.now()-t1)\n",
    "        t1 = datetime.now()\n",
    "\n",
    "# populate upper triangular matrix with transposition of lower triangular matrix\n",
    "i_upper = np.triu_indices(N_USERS, -1)\n",
    "weights[i_upper] = weights.T[i_upper]\n",
    "\n",
    "# sanity checks:\n",
    "print(weights[15, 15]) # must be equal to 1, since corr(x,x)=1\n",
    "print(weights[10, 15] == weights[15, 10]) # must return True, since corr(x,y)=corr(y,x)\n",
    "\n",
    "# save the weights for prediction\n",
    "np.save('./weights', weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.594078171940206\n",
      "Test MSE: 0.6304054522802464\n"
     ]
    }
   ],
   "source": [
    "weights = np.load(\"./weights.npy\")\n",
    "\n",
    "# select K top neighbors for each user\n",
    "#N_NEIGH = 50\n",
    "#neigh = {}\n",
    "#for user in weights.index:\n",
    "#    neigh[user] = weights.loc[user].sort_values(ascending=False).loc[:N_NEIGH]\n",
    "\n",
    "\n",
    "# PREDICTION\n",
    "def predict(i=10, j=23):\n",
    "    who_watched = np.argwhere(~np.isnan(R_mat[:, j]))\n",
    "    numer = (weights[i, who_watched].reshape(-1,1) * (R_mat[who_watched, j])).sum()\n",
    "    denom = (np.abs(weights[i, who_watched])).sum()\n",
    "    s_ij = mean_ratings[i] + numer / denom\n",
    "    # round extreme results so the range of ratings is <0.5;5>\n",
    "    s_ij = min(5, s_ij)\n",
    "    s_ij = max(0.5, s_ij)\n",
    "    return s_ij\n",
    "\n",
    "# test\n",
    "#predict(i=0, j=0)\n",
    "\n",
    "# defining MSE function\n",
    "def MSE(true, pred):\n",
    "    return np.sum(np.subtract(true, pred)**2) / len(pred)\n",
    "\n",
    "\n",
    "# calculating MSE on train set, which pairs to choose?\n",
    "train_pred_pairs = []\n",
    "for ind, row in X_train.iterrows():\n",
    "    tmp = (user_map_rev[row['userId']], movies_map_rev[row['movieId']])\n",
    "    train_pred_pairs.append(tmp)\n",
    "\n",
    "# calculate score for train set, for 50k ratings only, I had some OutOfMemory errors on my laptop\n",
    "train_scores = np.array([predict(x, y) for x, y in train_pred_pairs[:50000]]).reshape(-1,1)\n",
    "true_train = np.array(X_train['rating'].values.reshape(-1,1)[:50000])\n",
    "train_MSE = MSE(true_train, train_scores)\n",
    "print(\"Train MSE:\", train_MSE[0])\n",
    "\n",
    "# calculating MSE on test set\n",
    "test_pred_pairs = []\n",
    "for ind, row in X_test.iterrows():\n",
    "    tmp = (user_map_rev[row['userId']], movies_map_rev[row['movieId']])\n",
    "    test_pred_pairs.append(tmp)\n",
    "\n",
    "test_scores = np.array([predict(x, y) for x, y in test_pred_pairs[:50000]]).reshape(-1,1)\n",
    "true_test = np.array(X_test['rating'].values.reshape(-1,1)[:50000])\n",
    "test_MSE = MSE(true_test, test_scores)\n",
    "print(\"Test MSE:\", test_MSE[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another algorithm widely used in a domain of recommender systems is matrix factorization. It is still applied on the same user-item ratings matrix as presented in case of collaborative filtering ($N$ users and $M$ items). The goal is to find two matrices $W$ and $U$ whose dot product yields the initial user-item matrix\n",
    "$$R=WU^T$$\n",
    ", with the following dimensionalities:\n",
    "* $R$ is of shape $NxM$\n",
    "* $W$ is of shape $NxK$\n",
    "* $U$ is of shape $MxK$\n",
    "\n",
    "However it is only possible the get the exact same matrix if $K=N$. In practice $K$ ranges from 10 to 50 and we get the best possible approximation of matrix $R$.\n",
    "$$\\hat{R} \\approx WU^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K$ is a number of hidden factors, features that explain differences between items. In case of movies these could represent for example whether a movie:\n",
    "* is comedy, drama or horror\n",
    "* is black and white\n",
    "* features a specific actor, etc.\n",
    "\n",
    "We can identify these features by analyzing the items which score particularly high/low for a given feature.\n",
    "\n",
    "We should at all cost refrain from multiplying full matrices $W$ and $U$ when making prediction because we only need one row and one column in order to predict:\n",
    "$$\\hat{r}_{ij} = w_i^Tu_j$$\n",
    "The intuition behind this equation is the following: the product of two vectors $w_i$ and $u_j$ tells us how much user's $i$ preferences correlate with item's $j$ attributes. Let's pretend we have 5 hidden features: $K=5$ which can mean for example:\n",
    "* feature 1: Is di Caprio in the cast?\n",
    "* feature 2: Did the movie get any Oscars?\n",
    "* feature 3: Are there any superheroes involved?\n",
    "* feature 4: Is it a comedy?\n",
    "* feature 5: Is there a thread about romance?\n",
    "\n",
    "Our user $i$ has preferences which are described by vector $w_i = [1, 0.7, -0.9, 0.2, 1]$. This user watched movies *Titanic* and *Iron Man* whose attributes are described by vectors $u_{titanic}=[0.9, 1.7, -1.1, 0, 1.3]$ and $u_{ironman}=[-0.9, -1, 1, 0, -0.8]$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User i will probably like Titanic because the score is 4.4\n",
      "User i will probably dislike Iron Man because the score is -3.3\n"
     ]
    }
   ],
   "source": [
    "usr = np.array([1, 0.7, -0.9, 0.2, 1])\n",
    "mov_titanic = np.array([0.9, 1.7, -1.1, 0, 1.3])\n",
    "mov_ironman = np.array([-0.9, -1, 1, 0, -0.8])\n",
    "\n",
    "print(\"User i will probably like Titanic because the score is\", np.round(usr.dot(mov_titanic), 1))\n",
    "print(\"User i will probably dislike Iron Man because the score is\", np.round(usr.dot(mov_ironman), 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is how to calculate matrices $W$ and $U$? We want to minimize differences between elements of matrix $R$ and $\\hat{R}$ which gives us the following cost function:\n",
    "$$J = \\sum_{i,j \\in \\Omega} (r_{ij} - \\hat{r}_{ij})^2 =  \\sum_{i,j \\in \\Omega} (r_{ij} - w_i^Tu_j)^2$$\n",
    ", where $\\Omega_{ij}$ is a set of all pairs where user $i$ rated a movie $j$.\n",
    "\n",
    "Similarly to collaborative filtering we need to include a user as well as a movie bias - there are users who generally rate movies high/low and there are movies which are generally liked/disliked. The user bias will be denoted as $b_i$ and movie bias as $c_j$:\n",
    "$$\\hat{r}_{ij} = w_i^Tu_j+b_i+c_j$$\n",
    "The final cost function to optimize is thus equal to:\n",
    "$$J = \\sum_{i,j \\in \\Omega} (r_{ij} - w_i^Tu_j - b_i - c_j)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a minimum of such a function we take a derivative of $J$ with respect to our parameters which in this case are elements of matrices $w_i$ and $u_j$. The derivation of $w_i$ is calculated as follows:\n",
    "$$\\frac{\\partial J}{\\partial w_i} = 2\\sum_{j \\in \\Omega_i} (r_{ij} - w_i^Tu_j - b_i - c_j)(-u_j)=0$$\n",
    ", where $j \\in \\Omega_i$ signifies a set of all movies $j$ that user $i$ has watched since elements of $w_i$ depend only on those values.\n",
    "$$\\sum_{j \\in \\Omega_i}(w_i^Tu_j)u_j=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "The dot product of vectors is commutative (i.e. $a\\cdot b=b\\cdot a$) so we can further transform the equation:\n",
    "$$\\sum_{j \\in \\Omega_i}(u_j^Tw_i)u_j=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "The expression on the left-hand side in parenthesis is a scalar and since $scalar \\cdot vector = vector \\cdot scalar$ we have:\n",
    "$$\\sum_{j \\in \\Omega_i}u_j(u_j^Tw_i)=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "On the left-hand side the $w_i$ factor doesn't depend on the summation index $j$ so we simplify:\n",
    "$$(\\sum_{j \\in \\Omega_i}u_ju_j^T)w_i=\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all it takes to calculate $w_i$ is to multiply both sides by the inverse of $\\sum_{j \\in \\Omega_i}u_ju_j^T$ and the result is:\n",
    "$$w_i=(\\sum_{j \\in \\Omega_i}u_ju_j^T)^{-1}\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "\n",
    "Similar derivation may be performed for updates of vectors $u_j$, $b_i$ and $c_j$.\n",
    "\n",
    "As $w_i$ depends on $u_j$ and vice versa the optimization of such a function may be achieved with **Alternating Least Squares (ALS)**. The name of a method comes from the fact that we will update parameters of $U$ and $W$ in turns (alternate between updates). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we can add regularization to prevent overfitting by adding a penalty to the cost function for large parameters' values. The penalty term will be equal to:\n",
    "$$\\lambda (\\|W\\|^2+\\|U\\|^2+\\|b\\|^2+\\|c\\|^2)$$\n",
    ", where $\\lambda$ is a regularization hyperparameter.\n",
    "\n",
    "The final updates of parameters will be equal to:\n",
    "$$w_i=(\\sum_{j \\in \\Omega_i}u_ju_j^T + \\lambda I)^{-1}\\sum_{j \\in \\Omega_i}(r_{ij}-b_i -c_j )u_j$$\n",
    "$$u_i=(\\sum_{i \\in \\Omega_j}w_iw_i^T + \\lambda I)^{-1}\\sum_{i \\in \\Omega_j}(r_{ij}-b_i -c_j )w_i$$\n",
    "$$b_i=\\frac{1}{(1+\\lambda)|\\Omega_i|} \\sum_{j \\in \\Omega_i} (r_{ij}-w_i^Tu_j-c_j)$$\n",
    "$$c_j=\\frac{1}{(1+\\lambda)|\\Omega_j|} \\sum_{i \\in \\Omega_j} (r_{ij}-w_i^Tu_j-b_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user2mov dict: 0.14 MB occupied\n",
      "mov2user dict: 0.04 MB occupied\n",
      "usermov2rating dict: 80.0 MB occupied\n"
     ]
    }
   ],
   "source": [
    "# making dictionaries for faster lookup\n",
    "user2mov = {}\n",
    "mov2user = {}\n",
    "usermov2rating = {}\n",
    "\n",
    "for ind, us in user_map.items():\n",
    "    user2mov[ind] = [movies_map_rev[x] for x in ratings.loc[ratings.userId==us, 'movieId'].tolist()]\n",
    "\n",
    "for ind, mo in movies_map.items():\n",
    "    mov2user[ind] = [user_map_rev[x] for x in ratings.loc[ratings.movieId==mo, 'userId'].tolist()]\n",
    "\n",
    "for ind, row in ratings.iterrows():\n",
    "    usermov2rating[(user_map_rev[int(row.userId)], movies_map_rev[int(row.movieId)])] = row.rating\n",
    "    \n",
    "print(f'user2mov dict: {np.round(sys.getsizeof(user2mov)/1024**2, 2)} MB occupied')\n",
    "print(f'mov2user dict: {np.round(sys.getsizeof(mov2user)/1024**2, 2)} MB occupied')\n",
    "print(f'usermov2rating dict: {np.round(sys.getsizeof(usermov2rating)/1024**2, 2)} MB occupied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing randomly matrices/vectors to optimize\n",
    "K = 15 \n",
    "W = np.random.randn(N_USERS, K)\n",
    "b = np.random.randn(N_USERS, 1)\n",
    "U = np.random.randn(N_MOVIES, K)\n",
    "c = np.random.randn(N_MOVIES, 1)\n",
    "miu = ratings.rating.mean() # centering of data, change for X_train only in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it's hard to fully vectorize calculation of loss function let's create an auxiliary dictionary which for a movie will return a list of pairs (user, rating) giving us all users who watched and rated a given movie. As there are far fewer movies than users, for *for loop* will be calculated much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to calculate loss\n",
    "unique_movies = ratings.movieId.unique()\n",
    "mov2user_rating = {}\n",
    "\n",
    "for movie in unique_movies:\n",
    "    tmp = ratings[ratings.movieId == movie]\n",
    "    usr_rat = (np.asarray([user_map_rev[x] for x in tmp.userId.values]),\n",
    "               np.asarray(tmp.rating.values))\n",
    "    mov2user_rating[movies_map_rev[movie]] = usr_rat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function takes a dict in the form of: movieId: [(user1, rating1), (user2, rating2), ...]\n",
    "def calc_loss(mov2ur = mov2user_rating):\n",
    "    count = 0\n",
    "    error = 0.\n",
    "    t1 = datetime.now()\n",
    "    for mov, info in mov2ur.items():\n",
    "        user, rat = info\n",
    "        \n",
    "        pred = W[user].dot(U[mov]) + b[user].flatten() + c[mov] + miu\n",
    "        diff = rat - pred\n",
    "        count += len(rat)\n",
    "        error += diff.dot(diff)\n",
    "    return error/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 MSE: 17.98201\n",
      "epoch 0 finished in 0:01:34.735402\n",
      "epoch 1 MSE: 0.74093\n",
      "epoch 1 finished in 0:01:39.411124\n",
      "epoch 2 MSE: 0.52566\n",
      "epoch 2 finished in 0:01:39.589653\n",
      "epoch 3 MSE: 0.49493\n",
      "epoch 3 finished in 0:01:34.344840\n",
      "epoch 4 MSE: 0.48472\n",
      "epoch 4 finished in 0:01:32.864324\n",
      "epoch 5 MSE: 0.48066\n",
      "epoch 5 finished in 0:01:31.365131\n",
      "epoch 6 MSE: 0.47862\n",
      "epoch 6 finished in 0:01:30.565071\n",
      "epoch 7 MSE: 0.47751\n",
      "epoch 7 finished in 0:01:30.406181\n",
      "epoch 8 MSE: 0.47687\n",
      "epoch 8 finished in 0:01:33.663267\n",
      "epoch 9 MSE: 0.47648\n",
      "epoch 9 finished in 0:01:37.379463\n",
      "epoch 10 MSE: 0.47622\n",
      "epoch 10 finished in 0:01:42.257707\n",
      "epoch 11 MSE: 0.47601\n",
      "epoch 11 finished in 0:01:30.311258\n",
      "epoch 12 MSE: 0.47582\n",
      "epoch 12 finished in 0:01:30.141258\n",
      "epoch 13 MSE: 0.47562\n",
      "epoch 13 finished in 0:01:30.228012\n",
      "epoch 14 MSE: 0.47537\n",
      "epoch 14 finished in 0:01:30.061576\n",
      "epoch 15 MSE: 0.47504\n",
      "epoch 15 finished in 0:01:29.990188\n",
      "epoch 16 MSE: 0.47462\n",
      "epoch 16 finished in 0:01:29.945473\n",
      "epoch 17 MSE: 0.47414\n",
      "epoch 17 finished in 0:01:30.191065\n",
      "epoch 18 MSE: 0.47369\n",
      "epoch 18 finished in 0:01:29.836084\n",
      "epoch 19 MSE: 0.47332\n",
      "epoch 19 finished in 0:01:33.996190\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATING LEAST SQUARES ALGORITHM\n",
    "\n",
    "K = 15 # setting K - latent dimensionality\n",
    "LAMBDA = 10 # regularization parameter\n",
    "N_EPOCHS = 20\n",
    "train_loss = []\n",
    "# test_loss = [] # implement in the future\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    # calculate MSE after epoch/20 epochs\n",
    "    epoch_mse = calc_loss(mov2user_rating)\n",
    "    train_loss.append(epoch_mse)\n",
    "    \n",
    "    t1 = datetime.now()\n",
    "    # update of W matrix (users)\n",
    "    for us, us_m in user2mov.items():\n",
    "        mat = U[us_m].T.dot(U[us_m]) + LAMBDA * np.eye(K)\n",
    "        us_mov_pairs = list(zip([us]*len(us_m), us_m))\n",
    "        vec = np.array([(usermov2rating[(x, y)] - b[x] - c[y] - miu)*(U[y].T) for x, y in us_mov_pairs]).sum(0)\n",
    "        W[us, :] = np.linalg.solve(mat, vec)\n",
    "        \n",
    "        # update user bias b\n",
    "        denom = 1.0 / (len(us_m) + LAMBDA)\n",
    "        numer = np.sum([usermov2rating[(us, mov)] - W[us].dot(U[mov].T) - c[mov] - miu for mov in us_m])\n",
    "        b[us] = numer*denom\n",
    "    \n",
    "    # update of U matrix (movies)\n",
    "    for mo, mo_u in mov2user.items():\n",
    "        mat = W[mo_u].T.dot(W[mo_u]) + LAMBDA * np.eye(K)\n",
    "        mov_us_pairs = list(zip([mo]*len(mo_u), mo_u))\n",
    "        vec = np.array([(usermov2rating[(y, x)] - b[y] - c[x] - miu)*(W[y].T) for x, y in mov_us_pairs]).sum(0)\n",
    "        U[mo, :] = np.linalg.solve(mat, vec)\n",
    "        \n",
    "        # update movie bias c\n",
    "        denom = 1.0 / (len(mo_u) + LAMBDA)\n",
    "        numer = np.sum([usermov2rating[(us, mo)] - W[us].dot(U[mo].T) - b[us]-miu for us in mo_u])\n",
    "        c[mo] = numer*denom\n",
    "    \n",
    "    print(\"epoch\", epoch, \"MSE:\", np.round(epoch_mse, 5))\n",
    "    print(\"epoch\", epoch, \"finished in\", datetime.now()-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot how the training loss evolved across epochs of training. Before first epoch the MSE was around $18$ (for random weights). MSE decreased with every epoch and after 5th epoch the decrease was tiny. The final MSE settled at $0.47$ which means that on average we are wrong with every prediction by $\\sqrt{0.47}=0.68$ stars. One could make easy-to-implement improvements to algorithm by constraining the predictions to range $0.5-5$ and try to round the predictions to the nearest half of rating (e.g. $3.65\\approx3.5$) to see whether it improves performance.\n",
    "\n",
    "The more data we have the better the quality of the algorithm. As the training was done only on the fraction of entire dataset (1.4 million out of 20 million available) we can expect to get better model using all available data. With such a large dataset we could use PySpark and create a cluster of EC2 instances on AWS to accelerate the training and make use of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE value\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
